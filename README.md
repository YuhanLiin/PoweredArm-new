# UW Powered Arm

This repo consists of two components. **Host**, which runs on your laptop/desktop, does the work of training the classifier used for our gesture recognition. **Target**, which runs on whatever embedded target we'll be using, receives gesture data and makes gesture predictions using the trained classifier.

## Host
Requires:
- **Python >3.6**
- **Pip**, for downloading dependencies. Not strictly necessary, but otherwise you'd need to install each required Python package manually.

Setup:
- `cd host`
- `pip install -r requirements.txt` to get dependencies.
- `python -m run`, which will display the help text for the main script.

If you want to run the tests:
- `pip install -r requirements_dev.txt`
- `pytest tests`. Add the `--train` flag to run the more time-consuming integration tests.

The script `run.py` is a command-line tool that allows you to train your classifiers using CSV training data, save classifiers into its own files, and use existing classifiers to generate header files used by the target application. It is the entry point for user interaction with the system.

### Data Layout
- `data/classifiers/` contains the classifiers trained and saved by the command-line tool. Filenames are based on the date and time of the training and the classifier accuracy.
- `data/dataset` contains the CSV training data we use for training classifiers.
- `out/` contains the header file generated by the classifier.

## Target
Requires:
- **ESP-IDF 4** installation instructions here: https://docs.espressif.com/projects/esp-idf/en/latest/get-started/index.html.

Run "make flash" to compile and flash the code onto the board and "make monitor" to look at the outputs.
