# UW Powered Arm

This repo consists of two components. **Host**, which runs on your laptop/desktop, does the work of training the classifier used for our gesture recognition. **Target**, which runs on whatever embedded target we'll be using, receives gesture data and makes gesture predictions using the trained classifier.

## Host
Requires:
- **Python >3.6**
- **Pip**, for downloading dependencies. Not strictly necessary, but otherwise you'd need to install each required Python package manually.

Setup:
- `cd host`
- `pip install -r requirements.txt` to get dependencies.
- `python -m run`, which will display the help text for the main script.

If you want to run the tests:
- `pip install -r requirements_dev.txt`
- `pytest tests`. Add the `--train` flag to run the more time-consuming integration tests.

The script `run.py` is a command-line tool that allows you to train your classifiers using CSV training data, save classifiers into its own files, and use existing classifiers to generate header files used by the target application. It is the entry point for user interaction with the system.

### Data Layout
- `data/classifiers/` contains the classifiers trained and saved by the command-line tool. Filenames are based on the date and time of the training and the classifier accuracy.
- `data/dataset` contains the CSV training data we use for training classifiers.
- `out/` contains the header file generated by the classifier.

## Target
Requires:
- **GCC**. Windows users will probably need something like MinGW to get it.
- **Python >3.6**

To run the code on target, run `python build.py target` in target folder. The code will use the header generated by the host component, so make sure you generate it beforehand. This command is not yet fully implemented.

To run the tests, run `python build.py test`. This will build the test code using a dummy header and run the tests.
